{"cells":[{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# !pip install snntorch\n","# !pip install tonic"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["import tonic\n","from tonic import DiskCachedDataset\n","import tonic.transforms as transforms\n","import torch\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from sklearn import linear_model\n","import time\n","import lsm_weight_definitions as lsm_wts\n","import lsm_models"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["loss_fn = torch.nn.CrossEntropyLoss()"]},{"cell_type":"code","execution_count":116,"metadata":{},"outputs":[],"source":["def train_one_epoch(model, epoch_index, training_loader, optimizer, loss_fn):\n","    running_loss = 0.\n","    last_loss = 0.\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(device)\n","    model.to(device)\n","    print(next(model.parameters()).is_cuda)\n","    for i, data in enumerate(training_loader):\n","        inputs, labels = data\n","        print(labels.shape)\n","        print(type(inputs))\n","        print(inputs.shape)\n","        optimizer.zero_grad()\n","        inputs = torch.reshape(inputs, (inputs.shape[1], inputs.shape[0], -1)).to('cuda:0')\n","        print(inputs.shape)\n","        print(inputs.device)\n","        outputs = model(inputs)\n","        # outputs.to(device)\n","        loss = loss_fn(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        if i % 25 == 24:\n","            last_loss = running_loss / 1000 # loss per batch\n","            print('  batch {} loss: {}'.format(i + 1, last_loss))\n","            # tb_x = epoch_index * len(training_loader) + i + 1\n","            # tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n","            running_loss = 0.0\n","\n","    return last_loss"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":712382,"status":"ok","timestamp":1694511859056,"user":{"displayName":"Anmol Biswas","userId":"10510465683504288355"},"user_tz":-330},"id":"iSeg9V5KfRZZ","outputId":"cf22e9b0-4310-4be2-cf2d-618168effe42"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","torch.Size([310, 256, 2312])\n","train batches completed:  24\n","train batches completed:  49\n","train batches completed:  74\n","train batches completed:  99\n","train batches completed:  124\n","train batches completed:  149\n","train batches completed:  174\n","train batches completed:  199\n","train batches completed:  224\n","running time of training epoch:  160.4107174873352 seconds\n","test batches completed:  24\n","(60000, 1000)\n","(10000, 1000)\n","(60000, 2312)\n","(10000, 2312)\n","mean in spiking (train) :  0.004515322\n","mean in spiking (test) :  0.0045424583\n","mean LSM spiking (train) :  0.1648524\n","mean LSM spiking (test) :  0.16819212\n","training linear model:\n","test score = 0.9605\n"]}],"source":["if __name__ == \"__main__\":\n","\n","    #Load dataset (Using NMNIST here)\n","    sensor_size = tonic.datasets.NMNIST.sensor_size\n","    frame_transform = transforms.Compose([transforms.Denoise(filter_time=3000),\n","                                          transforms.ToFrame(sensor_size=sensor_size,time_window=1000)])\n","\n","    trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n","    testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n","\n","    cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n","    cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n","\n","    batch_size = 309\n","    trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n","    testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n","\n","    #Set device\n","    #device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n","    device = torch.device(\"cpu\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","    print(device)\n","\n","    data, targets = next(iter(trainloader))\n","    flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1))\n","    print(flat_data.shape)\n","\n","    in_sz = flat_data.shape[-1]\n","\n","    #Set neuron parameters\n","    tauV = 15.0\n","    tauI = 15.0\n","    th = 18\n","    curr_prefac = np.float32(1/tauI)\n","    alpha = np.float32(np.exp(-1/tauI))\n","    beta = np.float32(1 - 1/tauV)\n","\n","    Win, Wlsm = lsm_wts.initWeights1(25, 1.5, 0.13, in_sz)\n","    N = Wlsm.shape[0]\n","    lsm_net = lsm_models.LSM(N, in_sz, np.float32(curr_prefac*Win), np.float32(curr_prefac*Wlsm), alpha=alpha, beta=beta, th=th).to(device)\n","    lsm_net.eval()\n","    #Run with no_grad for LSM\n","    with torch.no_grad():\n","        start_time = time.time()\n","        for i, (data, targets) in enumerate(iter(trainloader)):\n","            if i%25 == 24:\n","                print(\"train batches completed: \", i)\n","            flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1)).to(device)\n","            spk_rec = lsm_net(flat_data)\n","            lsm_out = torch.mean(spk_rec, dim=0)\n","            if i==0:\n","                in_train = torch.mean(flat_data, dim=0).cpu().numpy()\n","                lsm_out_train = lsm_out.cpu().numpy()\n","                lsm_label_train = np.int32(targets.numpy())\n","            else:\n","                in_train = np.concatenate((in_train, torch.mean(flat_data, dim=0).cpu().numpy()), axis=0)\n","                lsm_out_train = np.concatenate((lsm_out_train, lsm_out.cpu().numpy()), axis=0)\n","                lsm_label_train = np.concatenate((lsm_label_train, np.int32(targets.numpy())), axis=0)\n","        end_time = time.time()\n","\n","        print(\"running time of training epoch: \", end_time - start_time, \"seconds\")\n","\n","        for i, (data, targets) in enumerate(iter(testloader)):\n","            if i%25 == 24:\n","                print(\"test batches completed: \", i)\n","            flat_data = torch.reshape(data, (data.shape[0], data.shape[1], -1)).to(device)\n","            lsm_net.eval()\n","            spk_rec = lsm_net(flat_data)\n","            lsm_out = torch.mean(spk_rec, dim=0)\n","            if i==0:\n","                in_test = torch.mean(flat_data, dim=0).cpu().numpy()\n","                lsm_out_test = lsm_out.cpu().numpy()\n","                lsm_label_test = np.int32(targets.numpy())\n","            else:\n","                in_test = np.concatenate((in_test, torch.mean(flat_data, dim=0).cpu().numpy()), axis=0)\n","                lsm_out_test = np.concatenate((lsm_out_test, lsm_out.cpu().numpy()), axis=0)\n","                lsm_label_test = np.concatenate((lsm_label_test, np.int32(targets.numpy())), axis=0)\n","\n","    print(lsm_out_train.shape)\n","    print(lsm_out_test.shape)\n","\n","    print(in_train.shape)\n","    print(in_test.shape)\n","\n","    print(\"mean in spiking (train) : \", np.mean(in_train))\n","    print(\"mean in spiking (test) : \", np.mean(in_test))\n","\n","    print(\"mean LSM spiking (train) : \", np.mean(lsm_out_train))\n","    print(\"mean LSM spiking (test) : \", np.mean(lsm_out_test))\n","\n","    print(\"training linear model:\")\n","    clf = linear_model.SGDClassifier(max_iter=10000, tol=1e-6)\n","    clf.fit(lsm_out_train, lsm_label_train)\n","\n","    score = clf.score(lsm_out_test, lsm_label_test)\n","    print(\"test score = \" + str(score))"]},{"cell_type":"code","execution_count":113,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","torch.Size([256, 311, 2312])\n","1000\n"]}],"source":["#Load dataset (Using NMNIST here)\n","sensor_size = tonic.datasets.NMNIST.sensor_size\n","frame_transform = transforms.Compose([transforms.Denoise(filter_time=3000),\n","                                        transforms.ToFrame(sensor_size=sensor_size,time_window=1000)])\n","\n","trainset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=True)\n","testset = tonic.datasets.NMNIST(save_to='./data', transform=frame_transform, train=False)\n","\n","cached_trainset = DiskCachedDataset(trainset, cache_path='./cache/nmnist/train')\n","cached_testset = DiskCachedDataset(testset, cache_path='./cache/nmnist/test')\n","\n","batch_size = 256\n","trainloader = DataLoader(cached_trainset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False), shuffle=True)\n","testloader = DataLoader(cached_testset, batch_size=batch_size, collate_fn=tonic.collation.PadTensors(batch_first=False))\n","\n","#Set device\n","#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","print(device)\n","\n","data, targets = next(iter(trainloader))\n","flat_data = torch.reshape(data, (data.shape[1], data.shape[0], -1))\n","print(flat_data.shape)\n","\n","in_sz = flat_data.shape[-1]\n","\n","#Set neuron parameters\n","tauV = 15.0\n","tauI = 15.0\n","th = 18\n","curr_prefac = np.float32(1/tauI)\n","alpha = np.float32(np.exp(-1/tauI))\n","beta = np.float32(1 - 1/tauV)\n","\n","Win, Wlsm = lsm_wts.initWeights1(25, 1.5, 0.13, in_sz)\n","N = Wlsm.shape[0]\n","print(N)\n","lsm_net = lsm_models.LSM(N, in_sz, np.float32(curr_prefac*Win), np.float32(curr_prefac*Wlsm), alpha=alpha, beta=beta, th=th).to(device)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[],"source":["optimizer = torch.optim.SGD(lsm_net.parameters(), lr=0.003, momentum=0.85)"]},{"cell_type":"code","execution_count":117,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH 1:\n","cuda\n","True\n","torch.Size([256])\n","<class 'torch.Tensor'>\n","torch.Size([311, 256, 2, 34, 34])\n","torch.Size([256, 311, 2312])\n","cuda:0\n"]},{"ename":"RuntimeError","evalue":"Expected target size [256, 1000], got [256]","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m lsm_net\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#train_one_epoch(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# trainloader.to(device)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m train_one_epoch(lsm_net,epoch_number, trainloader, optimizer, loss_fn)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss_arr\u001b[39m.\u001b[39mappend(avg_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m epoch_ind\u001b[39m.\u001b[39mappend(epoch_number\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n","\u001b[1;32m/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# outputs.to(device)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/umic/Desktop/SeDriCa/Students/Aryan_Mishra/snntorch-LSM-main/main.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.11/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n","File \u001b[0;32m~/anaconda3/envs/ML/lib/python3.11/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n","\u001b[0;31mRuntimeError\u001b[0m: Expected target size [256, 1000], got [256]"]}],"source":["import datetime\n","timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","epoch_number = 0\n","\n","EPOCHS = 10\n","lsm_net.to('cpu')\n","best_vloss = 1_000_000.\n","loss_arr = []\n","epoch_ind = []\n","for epoch in range(EPOCHS):\n","    print('EPOCH {}:'.format(epoch_number + 1))\n","\n","    # Make sure gradient tracking is on, and do a pass over the data\n","    lsm_net.train(True)\n","    #train_one_epoch(model, epoch_index, tb_writer, training_loader, optimizer, loss_fn):\n","    # trainloader.to(device)\n","    avg_loss = train_one_epoch(lsm_net,epoch_number, trainloader, optimizer, loss_fn)\n","    loss_arr.append(avg_loss)\n","    epoch_ind.append(epoch_number+1)\n","    running_vloss = 0.0\n","    # Set the model to evaluation mode, disabling dropout and using population\n","    # statistics for batch normalization.\n","    lsm_net.eval()\n","\n","    # Disable gradient computation and reduce memory consumption.\n","    with torch.no_grad():\n","        for i, vdata in enumerate(testloader):\n","            vinputs, vlabels = vdata\n","            voutputs = lsm_net(vinputs)\n","            vloss = loss_fn(voutputs, vlabels)\n","            running_vloss += vloss\n","\n","    avg_vloss = running_vloss / (i + 1)\n","    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n","\n","    # Log the running loss averaged per batch\n","    # for both training and validation\n","    # writer.add_scalars('Training vs. Validation Loss',\n","                    # { 'Training' : avg_loss, 'Validation' : avg_vloss },\n","                    # epoch_number + 1)\n","    # writer.flush()\n","\n","    # Track best performance, and save the model's state\n","    if avg_vloss < best_vloss:\n","        best_vloss = avg_vloss\n","        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n","        torch.save(lsm_net.state_dict(), model_path)\n","\n","    epoch_number += 1"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(epoch_ind, loss_arr)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMyvlJntnTUYFt04yX3UUrA","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
